# Image-Retrieval-and-Explanation-
This project implements an end-to-end image retrieval and explanation pipeline using CLIP, FAISS, BLIP, and Groq LLM. The system allows users to query with natural language, retrieve the most relevant images, generate captions, and provide explanations through a FastAPI backend with a simple HTML+JS frontend

## ðŸ“‚ Project Structure
'''
ðŸ“¦ project-root

â”œâ”€â”€ ðŸ“‚ app # FastAPI backend + frontend (HTML+JS)

 â””â”€â”€ ðŸ“‚ models # Pre-trained models (CLIP, BLIP, etc.)
 
 â””â”€â”€ ðŸ“„ image_index.faiss # Generated FAISS index
 
 â””â”€â”€ðŸ“„ metadata.json # Image metadata (id, image path)
 
 â””â”€â”€ ðŸ“„ requirements.txt # Python dependencies
 
 â””â”€â”€ ðŸ“„ Dockerfile # Container definition
 
 â””â”€â”€ ðŸ“‚ images # Image dataset

â”œâ”€â”€ ðŸ“‚ preprocessing # Scripts for dataset download & embedding generation
'''
## Architecture
The system consists of two main pipelines:
1. Preprocessing Pipeline
   â€¢	CLIP Encoder: Encodes text queries into embeddings.
   â€¢	FAISS Index: Stores image embeddings for fast nearest neighbor search.
   â€¢	Metadata Store: Maintains mapping of image IDs and paths for retrieval.
   <img width="728" height="251" alt="image" src="https://github.com/user-attachments/assets/fdc22918-f156-46dd-b52b-6e98f6dbc3d0" />

2. Image Retrieval & Explanation Flow
   <img width="644" height="448" alt="image" src="https://github.com/user-attachments/assets/6f138b1b-d476-4d17-a4c5-d82802b4465c" />
   
   1.	User enters a query on the web UI.
   2.	Frontend sends a POST /search request with JSON {query, top_k}.
   3.	FastAPI encodes the query with CLIP and retrieves top-K matches from FAISS.
   4.	BLIP generates captions for each image.
   5.	Groq LLM provides explanations for why the image matches.
   6.	API responds with:{"results": [ {"image_path": "...","caption": "...","explanation": "..."} ] }
   7.	Frontend renders results as image cards.







